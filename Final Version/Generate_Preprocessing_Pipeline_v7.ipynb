{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions definition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing functions :\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_columns(df):\n",
    "    \"\"\"Drop les colonnes définies ci dessous\n",
    "    \n",
    "    Retourne le df modifié\n",
    "    \"\"\"    \n",
    "    \n",
    "    columns_to_drop = ['piezo_station_commune_code_insee', 'piezo_station_department_code', 'meteo_name', \n",
    "                        'piezo_station_bss_id', 'prelev_structure_code_0', 'prelev_structure_code_1', \n",
    "                        'prelev_structure_code_2', 'piezo_station_pe_label', 'hydro_method_label','piezo_status',\n",
    "                         'piezo_station_bdlisa_codes','hydro_station_code','piezo_station_department_name', \n",
    "                         'piezo_station_commune_name', 'piezo_measure_nature_name', 'meteo_name', 'hydro_status_label', \n",
    "                         'piezo_station_bss_code', 'piezo_bss_code', 'piezo_producer_name', 'meteo_id','hydro_method_code','piezo_continuity_code',\n",
    "                         'hydro_hydro_quantity_elab']\n",
    "\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CleanNA_preprocessing(dataset_input,CategoricalAddNA_threshold=15, numericalCleanNA_threshhold=70, numerical_mode=\"median\"):\n",
    "    \"\"\"\n",
    "    input :\n",
    "        - ategoricalAddNA_threshold = 15\n",
    "        - numericalCleanNA_threshhold = 70\n",
    "        - numerical_mode=\"median\"\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_output = dataset_input\n",
    "\n",
    "    ## convert 4 insee collumns to float :\n",
    "    dataset_output[\"insee_%_const\"] = pd.to_numeric(dataset_output[\"insee_%_const\"], errors='coerce')\n",
    "    dataset_output[\"insee_%_ind\"] = pd.to_numeric(dataset_output[\"insee_%_ind\"], errors='coerce')\n",
    "    dataset_output[\"insee_med_living_level\"] = pd.to_numeric(dataset_output[\"insee_med_living_level\"], errors='coerce')\n",
    "    dataset_output[\"insee_%_agri\"] = pd.to_numeric(dataset_output[\"insee_%_agri\"], errors='coerce')\n",
    "\n",
    "    ## Variables categorical :\n",
    "    # Liste des colonnes avec des valeurs manquantes à traiter\n",
    "    columns_to_impute = [\n",
    "        'prelev_volume_obtention_mode_label_2', 'prelev_usage_label_2',\n",
    "        'prelev_usage_label_1', 'prelev_volume_obtention_mode_label_1',\n",
    "        'prelev_usage_label_0', 'prelev_volume_obtention_mode_label_0',\n",
    "        'piezo_measure_nature_code'\n",
    "    ]\n",
    "    for col in columns_to_impute:\n",
    "        # Calculate the percentage of missing values in the column\n",
    "        percentage_na = dataset_output[col].isna().mean() * 100\n",
    "        \n",
    "        if percentage_na > CategoricalAddNA_threshold:\n",
    "            # Replace all missing values with \"na\" if percentage > 15%\n",
    "            dataset_output[col].fillna(\"na\", inplace=True)\n",
    "        else:\n",
    "            # Otherwise, replace missing values with the mode\n",
    "            mode = dataset_output[col].mode()[0]  # Calculate the mode (most frequent value)\n",
    "            dataset_output[col].fillna(mode, inplace=True)\n",
    "\n",
    "\n",
    "    ## Variables numerical :\n",
    "    # Filter only numerical columns\n",
    "    numerical_data = dataset_output.select_dtypes(include=['float64', 'int64'])\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    missing_percentage = (numerical_data.isnull().sum() / len(dataset_output)) * 100\n",
    "    # Create a list of numerical columns with missing values\n",
    "    columns_with_missing_values = missing_percentage[missing_percentage > 0].index.tolist()\n",
    "    # Handle missing values: Drop columns with >=70% missing, otherwise fill with the median\n",
    "    for col in columns_with_missing_values:\n",
    "        if missing_percentage[col] >= numericalCleanNA_threshhold:  # Correctly reference the missing percentage of the specific column\n",
    "            dataset_output.drop(columns=[col], inplace=True)\n",
    "        else:\n",
    "            if numerical_mode == \"mean\":\n",
    "                dataset_output[col].fillna(dataset_output[col].mean(), inplace=True)\n",
    "            else:\n",
    "                dataset_output[col].fillna(dataset_output[col].median(), inplace=True)\n",
    "\n",
    "\n",
    "    return dataset_output\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def add_distance_from_algiers(dataset):\n",
    "\n",
    "    # Define Algiers coordinates (latitude and longitude in degrees)\n",
    "    algiers_latitude = 47.0812  # Latitude of Algiers\n",
    "    algiers_longitude = 2.3980  # Longitude of Algiers\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"\n",
    "        Calculate the great-circle distance between two points on the Earth using the Haversine formula.\n",
    "        \"\"\"\n",
    "        # Earth radius in kilometers\n",
    "        R = 6371.0  \n",
    "\n",
    "        # Convert latitude and longitude to radians\n",
    "        lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)\n",
    "        lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)\n",
    "\n",
    "        # Haversine formula\n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "        # Distance in kilometers\n",
    "        distance = R * c\n",
    "        return distance\n",
    "\n",
    "    # Apply the Haversine formula to calculate distance from Algiers for each row\n",
    "    dataset['distance_from_algiers'] = dataset.apply(\n",
    "        lambda row: haversine(\n",
    "            row['piezo_station_latitude'], row['piezo_station_altitude'],\n",
    "            algiers_latitude, algiers_longitude\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def DateTime_Preprocessing_importupdatefromCSV(dataset_input,piezo_station_update_date__csv_path = \"df_piezo_station_update_date.csv\"): #, piezo_station_update_date__csv_path=\"df_piezo_station_update_date.csv\"):\n",
    "    \"\"\"\n",
    "        input :\n",
    "            - piezo_station_update_date__csv_path = \"df_piezo_station_update_date.csv\" : path to csv of df_piezo_station_update_date collumn that massyl have\n",
    "        output :\n",
    "            - collumn : \"measurement_day_of_year\"\n",
    "            - collumn : \"measurement_year\"\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_output = dataset_input\n",
    "\n",
    "\n",
    "    ## drop \"hydro_observation_date_elab\" and \"meteo_date\"\n",
    "    dataset_output.drop(columns=[\"hydro_observation_date_elab\",\"meteo_date\",\"piezo_station_update_date\"],inplace=True)\n",
    "\n",
    "\n",
    "    ## to_datetime :\n",
    "    dataset_output['piezo_measurement_date'] = pd.to_datetime(dataset_output['piezo_measurement_date'])\n",
    "\n",
    "\n",
    "    ## Preprocess \"piezo_measurement_date\" :\n",
    "    # Calculate the fraction of the year for mm-dd\n",
    "    dataset_output['measurement_day_of_year'] = dataset_output['piezo_measurement_date'].dt.day_of_year  # Day number within the year (1-366)\n",
    "    dataset_output['measurement_day_of_year'] = dataset_output['measurement_day_of_year'] / 366  # Normalize to [0, 1]\n",
    "    # Extract the year\n",
    "    dataset_output['measurement_year'] = dataset_output['piezo_measurement_date'].dt.year\n",
    "    # Encode \"measurement_year\"\n",
    "    mapping = {\n",
    "        2020: 0,\n",
    "        2021: 1,\n",
    "        2022: 2,\n",
    "        2023: 3\n",
    "    }\n",
    "    dataset_output['measurement_year'] = dataset_output['measurement_year'].replace(mapping)\n",
    "    # drop the transformed \"piezo_measurement_date\"\n",
    "    dataset_output.drop(columns=[\"piezo_measurement_date\"],inplace=True)\n",
    "\n",
    "    return dataset_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode(df, encoding='label'):\n",
    "    \"\"\" encode les variables évidemment ordonnées en ordinal, les variables binaires et\n",
    "    (au choix) encode les variables non ordonnées soit via OHE soit via label encoding\n",
    "    \n",
    "    Variables\n",
    "    ---------\n",
    "    encoding (str) : 'label' pour label encoding, 'OHE' pour one hot (uniquement variables pertinentes)\n",
    "    \"\"\"\n",
    "\n",
    "    mappings_ord = {\n",
    "        'hydro_qualification_label' : {'Douteuse': 0, 'Non qualifiée': 1, \n",
    "                           'Bonne': 2},\n",
    "        'piezo_qualification' : {'Incorrecte': 0, 'Incertaine': 1, \n",
    "                           'Non qualifié': 2, 'Correcte': 3},\n",
    "        'piezo_obtention_mode' : {'Valeur mesurée':1, \"Mode d'obtention inconnu\":0,\n",
    "                                'Valeur reconstituée':-1}\n",
    "        \n",
    "        }\n",
    "    #binaire, à label encoder\n",
    "    columns_to_encode_binaire = ['hydro_hydro_quantity_elab',\n",
    "                                'piezo_continuity_name',]\n",
    "\n",
    "    # soit label-encode soit one hot-encode\n",
    "    columns_to_encode = ['piezo_measure_nature_code',\n",
    "                        'prelev_usage_label_0', \n",
    "                        'prelev_usage_label_1', \n",
    "                        'prelev_usage_label_2',\n",
    "                        'prelev_volume_obtention_mode_label_0', \n",
    "                        'prelev_volume_obtention_mode_label_1', \n",
    "                        'prelev_volume_obtention_mode_label_2',] #target\n",
    "    label_encoders = {}\n",
    "    \n",
    "    #ordinal, on ordonne\n",
    "    for col, mapping in mappings_ord.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping)\n",
    "    \n",
    "    # encoding au choix\n",
    "    if encoding == 'OHE':\n",
    "        columns_to_LE = columns_to_encode_binaire\n",
    "        #OHE\n",
    "        for col in columns_to_encode:\n",
    "            if col in df.columns:\n",
    "                one_hot = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "                df = pd.concat([df, one_hot], axis=1)\n",
    "                df = df.drop(columns=[col], errors='ignore') \n",
    "    else:\n",
    "        columns_to_LE = columns_to_encode_binaire + columns_to_encode\n",
    "        \n",
    "    #LE\n",
    "    for col in columns_to_LE:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_y(df):\n",
    "    df['piezo_groundwater_level_category'] = df['piezo_groundwater_level_category'].map({'Very Low': 0, 'Low': 1, 'Average': 2, 'High': 3, 'Very High': 4})\n",
    "    return df\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardise_preprocessing(dataset_input):\n",
    "    # Create a StandardScaler instance\n",
    "    scalers = {}\n",
    "\n",
    "    for column in dataset_input.columns:\n",
    "        scaler = StandardScaler()\n",
    "        # Standardize only numerical columns with more than 10 unique values\n",
    "        if dataset_input[column].dtype in ['int64', 'float64'] and dataset_input[column].nunique() > 10:\n",
    "            # Apply StandardScaler\n",
    "            dataset_input[column] = scaler.fit_transform(dataset_input[[column]])\n",
    "            scalers[column] = scaler\n",
    "\n",
    "    return dataset_input, scalers\n",
    "\n",
    "\n",
    "\n",
    "def standardise_preprocessing_test(dataset_input, scalers):\n",
    "    # Create a StandardScaler instance\n",
    "\n",
    "    for column in scalers.keys():\n",
    "        scaler = scalers[column]\n",
    "        # Standardize only numerical columns with more than 10 unique values\n",
    "        dataset_input[column] = scaler.transform(dataset_input[[column]])\n",
    "\n",
    "    return dataset_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def remove_colinear_collumns_preprocessing(dataset_input, correlation_threshold = 0.8) :\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = dataset_input.corr()\n",
    "\n",
    "    # Mask the upper triangle and the diagonal (convert mask to boolean)\n",
    "    mask = np.triu(np.ones(correlation_matrix.shape, dtype=bool), k=1)\n",
    "\n",
    "    # Apply the mask\n",
    "    filtered_corr = correlation_matrix.where(mask)\n",
    "\n",
    "    # Identify columns to drop\n",
    "    columns_to_drop = set()\n",
    "    for col1 in filtered_corr.columns:\n",
    "        for col2 in filtered_corr.columns:\n",
    "            if not np.isnan(filtered_corr.loc[col1, col2]) and abs(filtered_corr.loc[col1, col2]) > correlation_threshold:\n",
    "                # Add one of the columns to the drop list\n",
    "                columns_to_drop.add(col2)\n",
    "\n",
    "    # Drop the identified columns\n",
    "    dataset_input = dataset_input.drop(columns=columns_to_drop)\n",
    "\n",
    "    return dataset_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_low_variance_columns(dataset_input, variance_threshold=0.01):\n",
    "    # Calculate the variance for each column\n",
    "    variances = dataset_input.var()\n",
    "\n",
    "    # Identify columns to drop\n",
    "    columns_to_drop = [column for column in variances.index if variances[column] < variance_threshold]\n",
    "\n",
    "    # Drop the identified columns\n",
    "    dataset_input = dataset_input.drop(columns=columns_to_drop)\n",
    "\n",
    "    # print\n",
    "    print(\"columns_to_drop : \",columns_to_drop)\n",
    "\n",
    "    return dataset_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_y(df):\n",
    "    forward = {'Very Low': 0, 'Low': 1, 'Average': 2, 'High': 3, 'Very High': 4}\n",
    "    backward = {v: k for k, v in forward.items()}\n",
    "    df['piezo_groundwater_level_category'] = df['piezo_groundwater_level_category'].map(backward)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsample_summer_preprocessing(df):\n",
    "    \"\"\" prend un dataframe et dedouble les valeurs d'ete\"\"\"\n",
    "    #détecter ete\n",
    "\n",
    "    debut = (28 * 5) / 366  # on laisse un peu de jours fin mai\n",
    "    fin = (365 - (28 * 3)) / 366 # et début octobre\n",
    "\n",
    "    #index of the summer data\n",
    "    ix_summer = (df['measurement_day_of_year'] >= debut) & (df['measurement_day_of_year'] <= fin)\n",
    "\n",
    "    rows_to_duplicate = df.loc[ix_summer]\n",
    "    new_indexes = range(9999999, 9999999 - len(rows_to_duplicate), -1)\n",
    "    duplicated_rows = rows_to_duplicate.copy()\n",
    "    duplicated_rows.index = new_indexes\n",
    "    duplicated_rows.index.name = \"row_index\"\n",
    "    df = pd.concat([df, duplicated_rows])\n",
    "\n",
    "    return df, ix_summer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsample_summer_preprocessing_y(df, ix_summer):\n",
    "\n",
    "    rows_to_duplicate = df.loc[ix_summer]\n",
    "    new_indexes = range(9999999, 9999999 - len(rows_to_duplicate), -1)\n",
    "    duplicated_rows = rows_to_duplicate.copy()\n",
    "    duplicated_rows.index = new_indexes\n",
    "    duplicated_rows.index.name = \"row_index\"\n",
    "    df = pd.concat([df, duplicated_rows])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsample_plus2_summer_preprocessing(df):\n",
    "    \"\"\"Takes a dataframe and triples the summer values.\"\"\"\n",
    "    # Define summer period\n",
    "    debut = (28 * 5) / 366  # End of May\n",
    "    fin = (365 - (28 * 3)) / 366  # Beginning of October\n",
    "\n",
    "    # Index of the summer data\n",
    "    ix_summer = (df['measurement_day_of_year'] >= debut) & (df['measurement_day_of_year'] <= fin)\n",
    "\n",
    "    # Select rows to duplicate\n",
    "    rows_to_duplicate = df.loc[ix_summer]\n",
    "    \n",
    "    # Create two sets of duplicates with new indexes\n",
    "    duplicated_rows_1 = rows_to_duplicate.copy()\n",
    "    duplicated_rows_1.index = range(9999999, 9999999 - len(rows_to_duplicate), -1)\n",
    "    \n",
    "    duplicated_rows_2 = rows_to_duplicate.copy()\n",
    "    duplicated_rows_2.index = range(9999999 - len(rows_to_duplicate), 9999999 - (2 * len(rows_to_duplicate)), -1)\n",
    "    \n",
    "    # Concatenate the original dataframe with the two sets of duplicates\n",
    "    df = pd.concat([df, duplicated_rows_1, duplicated_rows_2])\n",
    "\n",
    "    return df, ix_summer\n",
    "\n",
    "\n",
    "def upsample_plus2_summer_preprocessing_y(df, ix_summer):\n",
    "    \"\"\"Upsample the target variable by a factor of 3.\"\"\"\n",
    "    # Select rows to duplicate\n",
    "    rows_to_duplicate = df.loc[ix_summer]\n",
    "    \n",
    "    # Create two sets of duplicates with new indexes\n",
    "    duplicated_rows_1 = rows_to_duplicate.copy()\n",
    "    duplicated_rows_1.index = range(9999999, 9999999 - len(rows_to_duplicate), -1)\n",
    "    \n",
    "    duplicated_rows_2 = rows_to_duplicate.copy()\n",
    "    duplicated_rows_2.index = range(9999999 - len(rows_to_duplicate), 9999999 - (2 * len(rows_to_duplicate)), -1)\n",
    "    \n",
    "    # Concatenate the original dataframe with the two sets of duplicates\n",
    "    df = pd.concat([df, duplicated_rows_1, duplicated_rows_2])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_out(df):\n",
    "    \"\"\"ete dans x_test : du 01/06 au 30/09\"\"\"\n",
    "    debut = (28 * 5) / 366  # on laisse un peu de jours fin mai\n",
    "    fin = (365 - (28 * 3)) / 366 # et début octobre\n",
    "\n",
    "    df = df[(df['measurement_day_of_year'] >= debut) & (df['measurement_day_of_year'] <= fin)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PreProcessing_full(dataset_X_train, dataset_X_test, CategoricalAddNA_threshold=15, numericalCleanNA_threshhold=70, filter_sum=False, encoding='label',correlation_threshold = 0.8, upsample_summer=0):\n",
    "    # Optional filtering for non-summer measurements\n",
    "    if filter_sum:\n",
    "        dataset_X_train = filter_out(dataset_X_train)  # Implement filter_out function\n",
    "        dataset_X_test = filter_out(dataset_X_test)\n",
    "\n",
    "    # Separate y_train from X_train\n",
    "    X_train = dataset_X_train.drop(columns=['piezo_groundwater_level_category'])  # Features\n",
    "    y_train = pd.DataFrame(dataset_X_train['piezo_groundwater_level_category'])  # Target\n",
    "\n",
    "    # Encode y_train\n",
    "    y_train_preprocessed = encode_y(y_train)  # Implement encode_y function\n",
    "\n",
    "    # save index\n",
    "    X_train_index = dataset_X_train.index\n",
    "    X_test_index = dataset_X_test.index\n",
    "\n",
    "    # Concatenate X_train and X_test\n",
    "    dataset_train_test = pd.concat([X_train, dataset_X_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    dataset_train_test = drop_columns(dataset_train_test)  # Implement drop_columns function\n",
    "    dataset_train_test = CleanNA_preprocessing(dataset_train_test, CategoricalAddNA_threshold, numericalCleanNA_threshhold)  # Implement CleanNA_preprocessing\n",
    "    dataset_train_test = add_distance_from_algiers(dataset_train_test)\n",
    "    dataset_train_test = DateTime_Preprocessing_importupdatefromCSV(dataset_train_test)  # Implement DateTime_Preprocessing\n",
    "    dataset_train_test = encode(dataset_train_test, encoding)  # Implement encode function\n",
    "    dataset_train_test, scalers = standardise_preprocessing(dataset_train_test)  # Implement standardise_preprocessing\n",
    "    dataset_train_test = remove_colinear_collumns_preprocessing(dataset_train_test, correlation_threshold)\n",
    "\n",
    "    # Split preprocessed dataset back into train and test sets\n",
    "    X_train_preprocessed = dataset_train_test.iloc[:len(X_train), :]  # Train part\n",
    "    X_test_preprocessed = dataset_train_test.iloc[len(X_train):, :]  # Test part\n",
    "\n",
    "    # Restore original index\n",
    "    X_train_preprocessed.index = X_train_index\n",
    "    X_test_preprocessed.index = X_test_index\n",
    "\n",
    "    # upsampling summer :\n",
    "    if upsample_summer == 1 :\n",
    "        X_train_preprocessed, ix_summer = upsample_summer_preprocessing(X_train_preprocessed)\n",
    "        y_train_preprocessed = upsample_summer_preprocessing_y(y_train_preprocessed, ix_summer)\n",
    "    elif upsample_summer == 2 :\n",
    "        X_train_preprocessed, ix_summer = upsample_plus2_summer_preprocessing(X_train_preprocessed)\n",
    "        y_train_preprocessed = upsample_plus2_summer_preprocessing_y(y_train_preprocessed, ix_summer)\n",
    "\n",
    "    # Save preprocessed data to CSV\n",
    "    X_train_preprocessed.to_csv('X_train_preprocessed.csv', sep=',', index=True)\n",
    "    y_train_preprocessed.to_csv('y_train_preprocessed.csv', sep=',', index=True)\n",
    "    X_test_preprocessed.to_csv('X_test_preprocessed.csv', sep=',', index=True)\n",
    "\n",
    "    return X_train_preprocessed, y_train_preprocessed, X_test_preprocessed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Preprocessing pipeline to generate the 3 CSVs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_X_train = pd.read_csv(\"X_train_Hi5.csv\",index_col=\"row_index\")\n",
    "dataset_X_test = pd.read_csv(\"X_test_Hi5.csv\",index_col=\"row_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed, y_train_preprocessed, X_test_preprocessed = PreProcessing_full(dataset_X_train, dataset_X_test, CategoricalAddNA_threshold=15, numericalCleanNA_threshhold=70, filter_sum=False, encoding='label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
